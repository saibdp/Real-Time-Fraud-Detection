{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e69776-5e7c-429f-ba6b-5167ab5489df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the secret dynamically from Azure Key Vault via Databricks secret scope\n",
    "service_credential =  dbutils.secrets.get(scope=\"dev_env\", key=\"adls-secret\") #scope name in DBX, secret name in keyvault\n",
    "\n",
    "# Azure AD and Storage configurations\n",
    "application_id = 'app_id'  \n",
    "directory_id = 'directory_id'  \n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": f\"{application_id}\",\n",
    "           \"fs.azure.account.oauth2.client.secret\": f\"{service_credential}\",\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f27d1c-f581-4d1f-b958-82acce1a9858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/saibdpadls/gold-layer has been unmounted.\n",
      "mount point already exists\n",
      "mount point re-mounted\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dbutils.fs.mount(\n",
    "    source = \"abfss://gold-layer@saibdpadls.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/saibdpadls/gold-layer\",\n",
    "    extra_configs = configs)\n",
    "\n",
    "except:\n",
    "    dbutils.fs.unmount(\"/mnt/saibdpadls/gold-layer\")\n",
    "    print(\"mount point already exists\")\n",
    "    dbutils.fs.mount(\n",
    "    source = \"abfss://gold-layer@saibdpadls.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/saibdpadls/gold-layer\",\n",
    "    extra_configs = configs)\n",
    "    print(\"mount point re-mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aafa521-5acf-48d2-8d8e-5b415bcca398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler has been saved to /dbfs/mnt/saibdpadls/ml-model/scaler.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7f84454580e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved to /dbfs/mnt/saibdpadls/ml-model/best_fraud_model.pkl\n",
      "Training columns have been saved to /dbfs/mnt/saibdpadls/ml-model/training_columns.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/dbfs/mnt/saibdpadls/gold-layer/synthetic_transactions.csv\") \n",
    "\n",
    "# Convert Timestamp to datetime and extract useful features\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
    "df[\"Month\"] = df[\"Timestamp\"].dt.month\n",
    "df.drop(columns=[\"Timestamp\", \"TransactionID\"], inplace=True)  # Drop original timestamp and ID\n",
    "\n",
    "# Convert Duration to seconds\n",
    "df[\"Duration\"] = df[\"Duration\"].apply(lambda x: sum(int(t) * sec for t, sec in zip(x.split(\":\"), [3600, 60, 1])))\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = [\"Location\", \"CardType\", \"TransactionType\", \"ProductCategory\"]\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Encode target variable (Benign = 0, Suspicious = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "df_encoded[\"Label\"] = label_encoder.fit_transform(df_encoded[\"Label\"])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X = df_encoded.drop(columns=[\"Label\"])\n",
    "y = df_encoded[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_file = \"/dbfs/mnt/saibdpadls/ml-model/scaler.pkl\"\n",
    "with open(scaler_file, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Scaler has been saved to {scaler_file}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "best_model = None\n",
    "best_false_negatives = float(\"inf\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # Select model with the lowest false negatives\n",
    "    if fn < best_false_negatives:\n",
    "        best_false_negatives = fn\n",
    "        best_model = model\n",
    "\n",
    "# Save the best model to a pickle file\n",
    "best_model_file = \"/dbfs/mnt/saibdpadls/ml-model/best_fraud_model.pkl\"\n",
    "with open(best_model_file, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"The best model has been saved to {best_model_file}\")\n",
    "\n",
    "\n",
    "training_columns_file = \"/dbfs/mnt/saibdpadls/ml-model/training_columns.pkl\"\n",
    "with open(training_columns_file, \"wb\") as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "\n",
    "print(f\"Training columns have been saved to {training_columns_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "649fcc0d-940f-441d-ae02-bc7c0cd70f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "syntheticdata",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
